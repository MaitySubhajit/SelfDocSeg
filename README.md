<div align="center">
	<br>
	<center><img src="./docs/static/images/banner.jpg" width=60% alt="Banner"></center>
	<br>
   <center>
      <a href="https://icdar2023.org">
      <picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://i0.wp.com/icdar2023.org/wp-content/uploads/2022/08/ICDAR-2023-Logo-white-v10-color.png?fit=1818%2C461&ssl=1">
      <source media="(prefers-color-scheme: light)" srcset="https://i0.wp.com/icdar2023.org/wp-content/uploads/2022/08/ICDAR-2023-Logo-v10-color.png">
      <img alt="icdar icon dual mode" src="https://i0.wp.com/icdar2023.org/wp-content/uploads/2022/08/ICDAR-2023-Logo-v10-color.png" width=20%>
      </picture>
      </a>
      <!-- <a href="https://icdar2023.org"><img src="https://i0.wp.com/icdar2023.org/wp-content/uploads/2022/08/ICDAR-2023-Logo-white-v10-color.png?fit=1818%2C461&ssl=1" width=20% alt="icdar banner"></a> -->
      <br>
      <br>
      <!-- <img src="https://img.shields.io/badge/Build-Passing-brightgreen?style=plastic" width=7.5% alt="Build shield"> -->
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-41676-7_20"><img src="https://img.shields.io/badge/-DOI-white?style=plastic&logo=data:image/svg+xml;base64,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" alt="springer shield"></a>
      <a href="https://arxiv.org/abs/2305.00795"><img src="https://img.shields.io/badge/-Paper-B31B1B?style=plastic&logo=arXiv" alt="arxiv shield"></a>
      <a href="https://maitysubhajit.github.io/SelfDocSeg/"><img src="https://img.shields.io/badge/-Project%20Page-222222?style=plastic&logo=githubpages" alt="github pages shield"></a>
      <a href="https://github.com/MaitySubhajit/SelfDocSeg"><img src="https://img.shields.io/badge/-Code-181717?style=plastic&logo=github" alt="github shield"></a>
      <br>
      <a href=""><img src="https://img.shields.io/badge/Build-Passing-brightgreen?style=plastic&logo=python" alt="build shield"></a>
      <a href=""><img src="https://img.shields.io/badge/Dependencies-Up%20to%20Date-brightgreen?style=plastic&logo=pytorch" alt="dependencies shield"></a>
      <a href="https://github.com/MaitySubhajit/SelfDocSeg/blob/main/LICENSE"><img src="https://img.shields.io/badge/&#128209%20License-GNU%20GPL%203.0-brightgreen?style=plastic" alt="license shield"></a>
      <br>
      <br>
      <br>
   </center>
</div>

# [ICDAR 2023] SelfDocSeg: A self-supervised vision-based approach towards Document Segmentation :fire:
This is the official implementation of the paper [SelfDocSeg: A self-supervised vision-based approach towards Document Segmentation](https://arxiv.org/pdf/2305.00795) by [S. Maity](https://maitysubhajit.github.io), [S. Biswas](https://www.linkedin.com/in/sanketbiswas), [S. Manna](https://sadimanna.github.io), [A. Banerjee](https://www.linkedin.com/in/ayan-banerjee-733b5b16b), [J. Lladós](http://www.cvc.uab.es/team/?action=profile&id=379), [S. Bhatttacharya](http://www.iitkgp.ac.in/department/EC/faculty/ec-saumik), [U. Pal](https://www.isical.ac.in/~umapada) to be published at [ICDAR 2023](https://icdar2023.org).

### :loudspeaker: Recent Updates
- *18 Aug 2023* : Pre-release is available! A stable release will be coming soon if required. If you face any problem, please read the [FAQ & Issues](#mag-faqs-notes-issues) section and raise an issue if necessary.

---

:pushpin: [Requirements](#rocket-requirements)

:pushpin: [Guidelines to Use](#clipboard-guidelines-to-use)

:pushpin: [FAQ & Issues](#mag-faqs-notes-issues)

:pushpin: [Acknowledgement](#star-acknowledgement)

:pushpin: [Citation](#bibtex)

### :wrench: Architecture
<div align="center"><center><img src="./docs/static/images/SelfDocSeg_icdar23_architecture_v4.png" width="100%"></center></div>

### :telescope: Qualitative Depiction of Segmentation
<div align="center"><center><img src="./docs/static/images/SelfDocSeg_qualitative.png" width="100%"></center></div>

### :bar_chart: Quantitative Performance Measures
|         Methods        |    Self-Supervision   | mAP on DocLayNet |
|:----------------------:|:---------------------:|:----------------:|
|  Supervised Mask RCNN  |                       |       72.4       |
|    BYOL + Mask RCNN    |:ballot_box_with_check:|       63.5       |
| SelfDocSeg + Mask RCNN |:ballot_box_with_check:|       74.3       |

## :rocket: Requirements
- Python 3.9
- torch 1.12.0, torchvision 0.13.0, torchaudio 0.12.0
- pytorch-lightning 1.8.1
- lightly 1.2.35
- torchinfo 1.7.1
- torchmetric 0.11
- tensorboard 2.11
- scipy 1.9.3
- numpy 1.23
- scikit-learn 1.1.3
- opencv-python 4.6
- pillow 9.3
- pandas 1.5
- seaborn 0.12.1
- matplotlib 3.6
- tabulate 0.9
- tqdm 4.64
- pyyaml 6.0
- yacs 0.1.8
- pycocotools 2.0
- detectron2 0.6

## :clipboard: Guidelines to Use
:white_check_mark: [Dataset](#Dataset)

:white_check_mark: [Pretraining](#pretraining)

:white_check_mark: [Finetuning](#finetuning)

:white_check_mark: [Visualization](#visualization)

### Dataset   
- For the self-supervised pretraining of SelfDocSeg we have used the [DocLayNet](https://developer.ibm.com/exchanges/data/all/doclaynet) dataset. It is also available for download in [:hugs: HuggingFace](https://huggingface.co/datasets/ds4sd/DocLayNet). The annotations are in COCO format. It should be extracted in the following structure. 
```
├── dataset                      # Dataset root directory
   ├── DocLayNet                 # DocLayNet dataset root directory
      ├── PNG                    # Directory containing all images
      |  ├── <image_file>.png
      |  ├── <image_file>.png
      |  ├── ...
      |
      └── COCO                   # Directory containing annotations
         ├── train.json          # train set annotation
         ├── val.json            # validation set annotation
         └── test.json           # test set annotation
```

- As DocLayNet is not a simple classification dataset we used a document classification dataset [RVL CDIP](https://adamharley.com/rvl-cdip), for linear evaluation protocols to make sure that the model is generalizing well. It is also available for download in [:hugs: HuggingFace](https://huggingface.co/datasets/aharley/rvl_cdip). The original dataset comes with separate image and annotation files. It needs to be restructured in `torchvision.datasets.ImageFolder` format as shown in the following, so that there are directories with each of the label names containing corresponding images for each of the dataset splits separately. We have used train + val split for training and test split for testing for kNN linear evaluation.
```
├── dataset                      # Dataset root directory
   ├── RVLCDIP_<split>           # RVL CDIP dataset split root directory, eg. 'RVLCDIP_train', 'RVLCDIP_test'
      ├── <label 0>              # Directory containing all images with label 0
      |  ├── <image_file>.tif
      |  ├── <image_file>.tif
      |  ├── ...
      |
      ├── <label 1>              # Directory containing all images with label 1
      |  ├── <image_file>.tif
      |  ├── <image_file>.tif
      |  ├── ...
      ├── <label 2>              # Directory containing all images with label 2
      |  ├── ...
      |
      ├── ...
      |
      └── <label 15>             # Directory containing all images with label 15
         ├── <image_file>.tif
         ├── <image_file>.tif
         ├── ...
```

### Pretraining
1. Run the script `pretraining/train_ssl.py` as `python pretraining/train_ssl.py --knn_train_root /path/to/train/ --knn_eval_root /path/to/test/ --dataset_root /path/to/pretraining/image/directory/` where `/path/to/train/`, `/path/to/test/` refer to RVL-CDIP kNN training split root directory 'RVLCDIP_train' and testing split root directory 'RVLCDIP_test' respectively and `/path/to/pretraining/image/directory/` refer to the DocLayNet image directory path. The complete set of options with default values is given below.
   ```
   python pretraining/train_ssl.py --num_eval_classes 16 --dataset_name DocLayNet --knn_train_root /path/to/train/ --knn_eval_root /path/to/test/ --dataset_root /path/to/pretraining/image/directory/ --logs_root ./benchmark_logs --num_workers 0 --max_epochs 800 --batchsize 8 --n_runs 1 --learning_rate 0.2 --lr_decay 5e-4 --wt_momentum 0.99 --bin_threshold 239 --kernel_shape rect --kernel_size 3 --kernel_iter 2 --eeta 0.001 --alpha 1.0 --beta 1.0
   ```
   If you want to resume training from a previous checkpoint add `--resume /path/to/checkpoint/` along with the command.

   To use multiple GPUs use `--distributed` flag and as additional controls, use `--sync_batchnorm` and `--gather_distributed` flags to synchronize batchnorms and gather features before loss calculation respectively across GPUs.

   Run `python pretraining/train_ssl.py --help` for the details.

2. The checkpoints and logs are being saved at `./benchmark_logs/<dataset_name>/version_<version num>/SelfDocSeg` directory. The `<version num>` depends on how many times the training is run and is automatically incremented from the largest `<version num>` available. If `--n_runs` passed is greater than 1, then `/run<run_number>` subdirectories are created to save data from each run. For the checkpoints, both the last epoch and the best kNN accuracy are the conditions to save weights in a subdirectory `checkpoints` under the aforementioned run directory.

3. Run `python pretraining/extract_encoder.py --checkpoint /path/to/saved/checkpont.ckpt --weight_save_path /path/to/save/weights.pth --num_eval_classes 16 --knn_train_root /path/to/train/ --knn_eval_root /path/to/test/ --dataset_root /path/to/pretraining/image/directory/` and the encoder weight will be extracted from the checkpoint and saved as the `.pth` file given in `save_path` in the default Torchvision ResNet50 format. The paths `/path/to/train/`, `/path/to/test/` refer to RVL-CDIP kNN training split root directory 'RVLCDIP_train' and testing split root directory 'RVLCDIP_test' respectively and `/path/to/pretraining/image/directory/` refer to the DocLayNet image directory path.

### Finetuning
1. Before finetuning the pretrained encoder on document segmentation, the weights need to be converted to the Detectron2 format by running the following.
   ```
   python finetuning/convert-torchvision-to-d2.py /path/to/save/weights.pth /path/to/save/d2/weights.pkl
   ```
   `/path/to/save/weights.pth` is the path to the extracted encoder weights from pretraining and `/path/to/save/d2/weights.pkl` is the file path where the converted weight file is to be saved in `.pkl` format.

2. Run the following command to start finetuning. The path `/path/to/DocLayNet/root/` refers to the root directory of the DocLayNet dataset in COCO format.
   ```
   python finetuning/train_net.py --num-gpus 1 --config-file finetuning/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml MODEL.WEIGHTS /path/to/save/d2/weights.pkl --dataset_root /path/to/DocLayNet/root/
   ```
   The training configuration is defined in `finetuning/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml` file and can be modified directly there or by parsing arguments in the command line. The path to the weights file can also be provided in the `.yaml` config file in the `WEIGHTS` key under `MODEL`.

   To train with multiple GPUs provide the number of available GPUs with `--num-gpus` argument. The learning rate and batch size might be required to be adjusted accordingly in the `.yaml` config file or in the command line, *eg.* `SOLVER.IMS_PER_BATCH 16 SOLVER.BASE_LR 0.02` for 8 GPUs.

3. The default path to save the logs and checkpoints is set in `finetuning/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml` file as `finetuning/output/doclaynet/mask_rcnn/rn50/`. The checkpoint after finetuning can be used to perform the evaluation on the DocLayNet dataset by adding `--eval-only` flag along with the checkpoint in the command line as below.
   ```
   python finetuning/train_net.py --config-file finetuning/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml --eval-only MODEL.WEIGHTS /path/to/finetuning/checkpoint.pkl
   ```

### Visualization
For visualization, run the following command. 
```
python visualize_json_results.py --input /path/to/output/evaluated/file.json --output /path/to/visualization/save/directory/ --dataset_root /path/to/DocLayNet/root/
```
The `/path/to/output/evaluated/file.json` refers to the `.json` file created during evaluation using Detectron2 in the output directory, default to `finetuning/output/doclaynet/mask_rcnn/rn50`. The `/path/to/visualization/save/directory/` refers to the directory path where the visualization results would be saved. The path `/path/to/DocLayNet/root/` refers to the root directory of the DocLayNet dataset in COCO format.

The confidence score threshold is set 0.6 by default and can be overridden by using `--conf-threshold 0.6` as an option in the command line.

## :mag: FAQs, Notes, Issues
- The `num_eval_classes 16` argument refers to the 16 classes in RVL-CDIP dataset used for linear evaluation.
- The pre-training can be done with any dataset by setting the pre-training dataset image folder by `--dataset_root /path/to/pretraining/image/directory/` and any dataset split in `torchvision.datasets.ImageFolder` format can be used for linear evaluation by using proper root paths to the split and number of classes, *eg.* `--num_eval_classes 16 --knn_train_root /path/to/train/ --knn_eval_root /path/to/test/`.
- The finetuning code in Detectron2 currently supports DocLayNet dataset only. If you wish to finetune on any other dataset, we recommend preparing the dataset in COCO format. Get help from [Detectron2 - Custom DataSet Tutorial](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html).
- The pretraining phase provides trained encoder weights in Torchvision format after extraction. Thus it can be used with any Mask RCNN implementation in PyTorch or any object detection framework instead of Detectron2.
- SelfDocSeg does not depend on textual guidance and hence can be used for documents of any language.

*If there is any query, please raise an issue. We shall try our best to help you out!*

## :star: Acknowledgement
The codes are implemented with the help of the two wonderful open-source repositories, [Lightly](https://github.com/lightly-ai/lightly) and [Detectron2](https://github.com/facebookresearch/detectron2).

## BibTeX  
If you use our code for your research, please cite our paper. Many thanks!

```
@inproceedings{maity2023selfdocseg,
title={SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation},
author={Subhajit Maity and Sanket Biswas and Siladittya Manna and Ayan Banerjee and Josep Lladós and Saumik Bhattacharya and Umapada Pal},
booktitle={International Conference on Document Analysis and Recognition (ICDAR)},
year={2023}}
```